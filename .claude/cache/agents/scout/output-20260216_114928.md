# MCP Mode Documentation

**Generated:** 2026-02-16 11:49:28

## Summary

MCP (Model Context Protocol) mode is a special operating mode in Cairo Coder that **returns raw documentation without LLM generation**. Instead of synthesizing an answer, it formats and returns retrieved documents directly. This is useful for integration with external tools that need raw Cairo/Starknet documentation.

**Key Difference:** Normal mode retrieves docs → generates answer via LLM. MCP mode retrieves docs → formats raw docs → returns.

---

## 1. Where MCP Mode is Defined/Triggered

### ✓ VERIFIED: API Routes (Entry Points)

**File:** `python/src/cairo_coder/server/app.py`

MCP mode is triggered via HTTP headers on three endpoints:

#### Lines 367-392: Agent-specific endpoint
```python
@self.app.post("/v1/agents/{agent_id}/chat/completions")
async def agent_chat_completions(
    agent_id: str,
    request: ChatCompletionRequest,
    req: Request,
    background_tasks: BackgroundTasks,
    mcp: str | None = Header(None),                    # Header 1: "mcp"
    x_mcp_mode: str | None = Header(None, alias="x-mcp-mode"),  # Header 2: "x-mcp-mode"
    vector_db: SourceFilteredPgVectorRM = Depends(get_vector_db),
    agent_factory: AgentFactory = Depends(get_agent_factory),
):
    # Determine MCP mode
    mcp_mode = bool(mcp or x_mcp_mode)  # Truthy if either header present
    
    return await self._handle_chat_completion(
        request, req, background_tasks, agent_factory, agent_id, mcp_mode, vector_db
    )
```

#### Lines 394-410: Legacy v1 endpoint
```python
@self.app.post("/v1/chat/completions")
async def v1_chat_completions(...):
    mcp_mode = bool(mcp or x_mcp_mode)
    return await self._handle_chat_completion(..., mcp_mode, ...)
```

#### Lines 412-428: Legacy endpoint
```python
@self.app.post("/chat/completions")
async def chat_completions(...):
    mcp_mode = bool(mcp or x_mcp_mode)
    return await self._handle_chat_completion(..., mcp_mode, ...)
```

**Trigger mechanism:** Either HTTP header activates MCP mode:
- `mcp: <any-truthy-value>`
- `x-mcp-mode: <any-truthy-value>`

---

### ✓ VERIFIED: Database Models

**File:** `python/src/cairo_coder/db/models.py` (Lines 16-30)

```python
class UserInteraction(BaseModel):
    """Represents a record in the user_interactions table."""
    
    id: uuid.UUID = Field(default_factory=uuid.uuid4)
    created_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
    agent_id: str
    mcp_mode: bool = False  # ← Stored for analytics/tracking
    conversation_id: Optional[str] = None
    user_id: Optional[str] = None
    chat_history: Optional[list[dict[str, Any]]] = None
    query: str
    generated_answer: Optional[str] = None
    retrieved_sources: Optional[list[RetrievedSourceData]] = None
    llm_usage: Optional[dict[str, Any]] = None
```

**File:** `python/src/cairo_coder/db/session.py` (Line 74)

```python
mcp_mode BOOLEAN NOT NULL DEFAULT FALSE,
```

MCP mode is persisted in the database for every interaction to track usage patterns.

---

### ✓ VERIFIED: Agent Factory

**File:** `python/src/cairo_coder/core/agent_factory.py` (Lines 42-69)

```python
def get_or_create_agent(
    self, agent_id: str, mcp_mode: bool = False
) -> RagPipeline:
    """
    Get an existing agent from cache or create a new one.
    
    Args:
        agent_id: Agent identifier
        mcp_mode: Whether to use MCP mode
    
    Returns:
        Cached or newly created RagPipeline instance
    """
    # Check cache first
    cache_key = f"{agent_id}_{mcp_mode}"  # ← Separate cache per mode
    if cache_key in self._agent_cache:
        return self._agent_cache[cache_key]
    
    # Get agent spec from registry
    _, spec = get_agent_by_string_id(agent_id)
    
    # Create new agent from spec
    agent = spec.build(self.vector_db, self.vector_store_config)
    
    # Cache the agent
    self._agent_cache[cache_key] = agent
```

**Important:** MCP mode affects caching. Each agent has two cached instances: one for normal mode, one for MCP mode.

---

## 2. What MCP Mode Does Differently

### ✓ VERIFIED: Core Pipeline Branching

**File:** `python/src/cairo_coder/core/rag_pipeline.py`

#### Non-streaming path (Lines 158-203)

```python
async def aforward(
    self,
    query: str,
    chat_history: list[Message] | None = None,
    mcp_mode: bool = False,  # ← Flag passed through pipeline
    sources: list[DocumentSource] | None = None,
) -> dspy.Prediction:
    """
    Execute the RAG pipeline and return a DSPy Prediction.
    
    Args:
        query: User's Cairo/Starknet programming question
        chat_history: Previous conversation messages
        mcp_mode: Return raw documents without generation  # ← KEY DIFFERENCE
        sources: Optional source filtering
    """
    chat_history_str = self._format_chat_history(chat_history or [])
    processed_query, documents, grok_citations = (
        await self._aprocess_query_and_retrieve_docs(query, chat_history_str, sources)
    )
    
    # BRANCHING POINT
    if mcp_mode:
        # MCP MODE: Use special program that formats docs without LLM generation
        result = await self.mcp_generation_program.acall(documents)
        return dspy.Prediction(
            processed_query=processed_query,
            documents=documents,
            grok_citations=grok_citations,
            answer=result.answer,  # ← Formatted raw docs
            formatted_sources=self._format_sources(documents, grok_citations),
        )
    
    # NORMAL MODE: Prepare context and generate with LLM
    context = self._prepare_context(documents)
    
    result = await self.generation_program.acall(
        query=query, context=context, chat_history=chat_history_str
    )
    
    return dspy.Prediction(
        processed_query=processed_query,
        documents=documents,
        grok_citations=grok_citations,
        answer=result.answer,  # ← LLM-generated answer
        formatted_sources=self._format_sources(documents, grok_citations),
    )
```

#### Streaming path (Lines 210-274)

```python
async def aforward_streaming(
    self,
    query: str,
    chat_history: list[Message] | None = None,
    mcp_mode: bool = False,
    sources: list[DocumentSource] | None = None,
) -> AsyncGenerator[StreamEvent, None]:
    """Execute the complete RAG pipeline with streaming support."""
    # ... retrieve documents ...
    
    final_answer: str | None = None
    
    if mcp_mode:
        # MCP mode: Return raw documents
        await _emit(
            StreamEvent(
                type=StreamEventType.PROCESSING,
                data="Formatting documentation...",
            )
        )
        
        mcp_prediction = await self.mcp_generation_program.acall(documents)
        final_answer = mcp_prediction.answer
        # Emit single response plus a final response event
        await _emit(
            StreamEvent(type=StreamEventType.RESPONSE, data=mcp_prediction.answer)
        )
```

**Key behavior differences:**

| Aspect | Normal Mode | MCP Mode |
|--------|-------------|----------|
| **LLM Usage** | Yes - calls `generation_program.acall()` | No - calls `mcp_generation_program.acall()` |
| **Context Preparation** | `self._prepare_context(documents)` | Skipped |
| **Output** | Synthesized answer from LLM | Formatted raw documents |
| **Streaming** | Token-by-token LLM streaming | Single formatted response |
| **Processing Message** | "Generating response..." | "Formatting documentation..." |

---

### ✓ VERIFIED: MCP Generation Program

**File:** `python/src/cairo_coder/dspy/generation_program.py` (Lines 289-360)

```python
class McpGenerationProgram(dspy.Module):
    """
    Special generation program for MCP (Model Context Protocol) mode.
    
    This program returns raw documentation without LLM generation,
    useful for integration with other tools that need Cairo documentation.
    """
    
    def __init__(self):
        super().__init__()
    
    def forward(self, documents: list[Document]) -> dspy.Prediction:
        """
        Format documents for MCP mode response.
        
        Args:
            documents: List of retrieved documents
        
        Returns:
            Formatted documentation string
        """
        if not documents:
            return dspy.Prediction(answer="No relevant documentation found.")
        
        formatted_docs = []
        for i, doc in enumerate(documents, 1):
            source = doc.source
            url = doc.source_link
            title = doc.title
            
            formatted_doc = f"""
## {i}. {title}

**Source:** {source}
**URL:** {url}

{doc.page_content}

---
"""
            formatted_docs.append(formatted_doc)
        
        return dspy.Prediction(answer="\n".join(formatted_docs))
    
    async def aforward(self, documents: list[Document]) -> dspy.Prediction:
        """Format documents for MCP mode response."""
        return self(documents)


def create_mcp_generation_program() -> McpGenerationProgram:
    """
    Factory function to create an MCP GenerationProgram instance.
    
    Returns:
        Configured McpGenerationProgram instance
    """
    return McpGenerationProgram()
```

**No LLM calls** - this is pure Python formatting. The program:
1. Takes retrieved documents
2. Formats each with markdown structure
3. Returns concatenated string

**Cost:** Near-zero (no LLM tokens consumed for generation)

---

### ✓ VERIFIED: Agent Registry Configuration

**File:** `python/src/cairo_coder/agents/registry.py` (Lines 104-136)

Every agent has an MCP generation program factory:

```python
def _create_mcp_generation_program() -> Any:
    """Factory for MCP generation program."""
    from cairo_coder.dspy.generation_program import create_mcp_generation_program
    
    return create_mcp_generation_program()


registry: dict[AgentId, AgentSpec] = {
    AgentId.CAIRO_CODER: AgentSpec(
        name="Cairo Coder",
        description="General Cairo programming assistant",
        sources=list(DocumentSource),
        pipeline_builder=RagPipelineFactory.create_pipeline,
        query_processor_factory=_create_query_processor,
        generation_program_factory=_create_cairo_coder_generation_program,
        mcp_generation_program_factory=_create_mcp_generation_program,  # ← MCP support
        max_source_count=MAX_SOURCE_COUNT,
        similarity_threshold=SIMILARITY_THRESHOLD,
    ),
    AgentId.STARKNET: AgentSpec(
        name="Starknet Agent",
        description="Assistant for the Starknet ecosystem",
        sources=list(DocumentSource),
        pipeline_builder=RagPipelineFactory.create_pipeline,
        query_processor_factory=_create_query_processor,
        generation_program_factory=_create_starknet_generation_program,
        mcp_generation_program_factory=_create_mcp_generation_program,  # ← MCP support
        max_source_count=MAX_SOURCE_COUNT,
        similarity_threshold=SIMILARITY_THRESHOLD,
    ),
}
```

**All agents** share the same `McpGenerationProgram` - it's agent-agnostic.

---

## 3. Code Flow: Request Path

### Normal Mode Flow

```
1. HTTP Request (no mcp headers)
   ↓
2. app.py: agent_chat_completions()
   - mcp_mode = False
   ↓
3. app.py: _handle_chat_completion()
   - Creates agent from factory
   - Calls _stream_chat_completion() or _generate_chat_completion()
   ↓
4. app.py: _stream_chat_completion()
   - Calls agent.aforward_streaming(query, history, mcp_mode=False)
   ↓
5. rag_pipeline.py: aforward_streaming()
   - Retrieves documents via query processor
   - Prepares context from documents
   - Calls generation_program.acall(query, context, history)
   ↓
6. generation_program.py: GenerationProgram
   - LLM synthesizes answer from context
   - Streams tokens back
   ↓
7. app.py: Streams tokens to client
   - Logs interaction with mcp_mode=False
```

### MCP Mode Flow

```
1. HTTP Request with header: x-mcp-mode: true
   ↓
2. app.py: agent_chat_completions()
   - mcp_mode = bool(mcp or x_mcp_mode) = True
   ↓
3. app.py: _handle_chat_completion()
   - Creates agent from factory (cached with key "agent_id_True")
   - Calls _stream_chat_completion() or _generate_chat_completion()
   ↓
4. app.py: _stream_chat_completion()
   - Calls agent.aforward_streaming(query, history, mcp_mode=True)
   ↓
5. rag_pipeline.py: aforward_streaming()
   - Retrieves documents via query processor
   - SKIPS context preparation
   - SKIPS LLM generation
   - Calls mcp_generation_program.acall(documents)
   ↓
6. generation_program.py: McpGenerationProgram
   - Formats documents as markdown (no LLM)
   - Returns formatted string
   ↓
7. app.py: Emits single RESPONSE event with formatted docs
   - Logs interaction with mcp_mode=True
```

### Key Differences Table

| Stage | Normal Mode | MCP Mode |
|-------|-------------|----------|
| **Trigger** | No special headers | `mcp` or `x-mcp-mode` header |
| **Agent Cache** | `agent_id_False` | `agent_id_True` |
| **Document Retrieval** | ✓ Same | ✓ Same |
| **Query Processing** | ✓ Same | ✓ Same |
| **Context Prep** | ✓ `_prepare_context()` | ✗ Skipped |
| **Generation** | `generation_program.acall()` | `mcp_generation_program.acall()` |
| **LLM Calls** | Multiple (query + generation) | One (query processing only) |
| **Output Format** | Synthesized answer | Raw docs with metadata |
| **Streaming** | Token-by-token | Single chunk |
| **Cost** | ~5-10k tokens | ~1-2k tokens |

---

## 4. Raw Context Returned

### ✓ VERIFIED: Output Format

**File:** `python/src/cairo_coder/dspy/generation_program.py` (Lines 313-329)

The MCP mode returns structured markdown for each document:

```markdown
## 1. Document Title

**Source:** DocumentSource.CAIRO_BOOK
**URL:** https://book.cairo-lang.org/ch01-01-hello-world.html

<Full document content from page_content field>

---

## 2. Second Document Title

**Source:** DocumentSource.STARKNET_DOCS
**URL:** https://docs.starknet.io/...

<Full document content>

---
```

**Structure per document:**
- Numbered heading with title
- Source enum value
- Source URL
- Full page content
- Horizontal rule separator

**Example output:**

```markdown
## 1. Getting Started with Cairo

**Source:** DocumentSource.CAIRO_BOOK
**URL:** https://book.cairo-lang.org/ch01-01-getting-started.html

Cairo is a programming language for writing provable programs...

[full content]

---

## 2. Smart Contracts on Starknet

**Source:** DocumentSource.STARKNET_DOCS
**URL:** https://docs.starknet.io/documentation/architecture_and_concepts/Smart_Contracts/

Starknet contracts are written in Cairo...

[full content]

---
```

### Document Fields Included

**File:** `python/src/cairo_coder/dspy/generation_program.py` (Lines 314-317)

```python
source = doc.source          # DocumentSource enum
url = doc.source_link       # String URL
title = doc.title           # String title
content = doc.page_content  # Full text content
```

### No Content Available Case

**Line 310-311:**
```python
if not documents:
    return dspy.Prediction(answer="No relevant documentation found.")
```

---

## 5. Usage in Optimizers

MCP mode is also used internally for DSPy optimization:

**File:** `python/src/cairo_coder/optimizers/generation_optimizer_cairo-coder.py` (Lines 59-75)

```python
# Create agent with MCP mode
documentation_fetcher = agent_factory.get_or_create_agent("cairo-coder", mcp_mode=True)

@dp.metric
async def relevance_metric(example: dspy.Example, prediction: dspy.Prediction) -> float:
    query = example.query
    generated_answer = prediction.answer
    
    # Fetch documentation context using MCP mode
    context_prediction = await documentation_fetcher.acall(query=query, mcp_mode=True)
    documentation_context = context_prediction.answer  # Raw docs
    
    # Evaluate with LLM
    evaluation = await evaluator.acall(
        query=query,
        generated_answer=generated_answer,
        documentation_context=documentation_context,
    )
    return evaluation.relevance_score
```

**Use case:** Fetch raw docs to use as ground truth for evaluation without double-generation overhead.

---

## Key Files Summary

| File | Purpose | Lines of Interest |
|------|---------|-------------------|
| `server/app.py` | API routes, MCP trigger | 367-428, 442-450, 508-648, 673-680 |
| `core/rag_pipeline.py` | Pipeline branching logic | 158-203, 210-274 |
| `dspy/generation_program.py` | MCP program implementation | 289-360 |
| `core/agent_factory.py` | Agent caching by mode | 42-69 |
| `agents/registry.py` | Agent MCP program factory | 104-136 |
| `db/models.py` | Interaction tracking | 16-30 |
| `db/session.py` | Database schema | 74 |
| `optimizers/generation_optimizer_*.py` | Internal MCP usage | 59-75 |

---

## Design Rationale

### Why MCP Mode Exists

1. **External Tool Integration**: Other tools (like MCP servers) need raw documentation, not synthesized answers
2. **Cost Efficiency**: Skip expensive LLM generation when raw docs are sufficient
3. **Evaluation**: Use as ground truth for DSPy optimization without circular dependencies
4. **Transparency**: Expose exactly what documents were retrieved for debugging

### Why It's Built This Way

1. **Shared Retrieval**: Both modes use same query processing and document retrieval for consistency
2. **Separate Programs**: `GenerationProgram` vs `McpGenerationProgram` keeps concerns separated
3. **Cache Separation**: Different cache keys prevent mode interference
4. **Header-Based**: HTTP headers allow easy client-side toggle without API changes

---

## Open Questions

None - MCP mode is fully documented and implemented consistently throughout the codebase.
